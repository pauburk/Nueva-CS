{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Synthetic Data\n",
    "\n",
    "For more explanation of logistic regression, see\n",
    "1. [Our course notes](https://jennselby.github.io/MachineLearningCourseNotes/#binomial-logistic-regression)\n",
    "1. [This scikit-learn explanation](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "1. [The full scikit-learn documentation of the LogisticRegression model class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "## Instructions\n",
    "0. If you haven't already, follow [the setup instructions here](https://jennselby.github.io/MachineLearningCourseNotes/#setting-up-python3) to get all necessary software installed.\n",
    "0. Read through the code in the following sections:\n",
    "  * [Data Generation](#Data-Generation)\n",
    "  * [Visualization](#Visualization)\n",
    "  * [Model Training](#Model-Training)\n",
    "  * [Prediction](#Prediction)\n",
    "0. Complete at least one of the exercise options:\n",
    "  * [Exercise Option #1 - Standard Difficulty](#Exercise-Option-#1---Standard-Difficulty)\n",
    "  * [Exercise Option #2 - Advanced Difficulty](#Exercise-Option-#2---Advanced-Difficulty)\n",
    "  * [Exercise Option #3 - Advanced Difficulty](#Exercise-Option-#3---Advanced-Difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random # for generating our dataset\n",
    "from sklearn import linear_model # for fitting our model\n",
    "\n",
    "# force numpy not to use scientific notation, to make it easier to read the numbers the program prints out\n",
    "numpy.set_printoptions(suppress=True)\n",
    "\n",
    "# to display graphs in this notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "As we did in the [linear regression notebook](https://nbviewer.jupyter.org/github/jennselby/MachineLearningCourseNotes/blob/master/assets/ipynb/LinearRegression.ipynb), we will be generating some fake data.\n",
    "\n",
    "In this fake dataset, we have two types of plants.\n",
    "* Plant A tends to be taller (average 60cm) and thinner (average 8cm).\n",
    "* Plant B tends to be shorter (average 58cm) and wider (average 10cm).\n",
    "* The heights and diameters of both plants are normally distributed (they follow a bell curve).\n",
    "\n",
    "* Class 0 will represent Plant A and Class 1 will represent Plant B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 50 # inputs per class\n",
    "PLANT_A_AVG_HEIGHT = 60.0\n",
    "PLANT_A_AVG_WIDTH = 8.0\n",
    "PLANT_B_AVG_HEIGHT = 58.0\n",
    "PLANT_B_AVG_WIDTH = 10.0\n",
    "\n",
    "# Pick numbers randomly with a normal distribution centered around the averages\n",
    "\n",
    "plant_a_heights = numpy.random.normal(loc=PLANT_A_AVG_HEIGHT, size=NUM_INPUTS)\n",
    "plant_a_widths = numpy.random.normal(loc=PLANT_A_AVG_WIDTH, size=NUM_INPUTS)\n",
    "\n",
    "plant_b_heights = numpy.random.normal(loc=PLANT_B_AVG_HEIGHT, size=NUM_INPUTS)\n",
    "plant_b_widths = numpy.random.normal(loc=PLANT_B_AVG_WIDTH, size=NUM_INPUTS)\n",
    "\n",
    "# this creates a 2-dimensional matrix, with heights in the first column and widths in the second\n",
    "# the first half of rows are all plants of type a and the second half are type b\n",
    "plant_inputs = list(zip(numpy.append(plant_a_heights, plant_b_heights),\n",
    "                        numpy.append(plant_a_widths, plant_b_widths)))\n",
    "\n",
    "# this is a list where the first half are 0s (representing plants of type a) and the second half are 1s (type b)\n",
    "classes = [0]*NUM_INPUTS + [1]*NUM_INPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's visualize our dataset, so that we can better understand what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a figure and label it\n",
    "fig = matplotlib.pyplot.figure()\n",
    "fig.suptitle('Plant Data Set')\n",
    "matplotlib.pyplot.xlabel('Height')\n",
    "matplotlib.pyplot.ylabel('Width')\n",
    "\n",
    "# put the generated points on the graph\n",
    "a_scatter = matplotlib.pyplot.scatter(plant_a_heights, plant_a_widths, c=\"red\", marker=\"o\", label='plant a')\n",
    "b_scatter = matplotlib.pyplot.scatter(plant_b_heights, plant_b_widths, c=\"blue\", marker=\"^\", label='plant b')\n",
    "\n",
    "# add a legend to explain which points are which\n",
    "matplotlib.pyplot.legend(handles=[a_scatter, b_scatter])\n",
    "\n",
    "# show the graph\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Next, we want to fit our logistic regression model to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [122.73284467]  Coefficients: [[-2.28533101  1.31101463]]\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(plant_inputs, classes)\n",
    "\n",
    "print('Intercept: {0}  Coefficients: {1}'.format(model.intercept_, model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Now we can make some predictions using the trained model. Note that we are generating the new data exactly the same way that we generated the training data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant A: 59.466689863321946 9.437983770836798\n",
      "Plant B: 57.016131587500176 10.730056481747866\n",
      "Class predictions: [0 1]\n",
      "Probabilities:\n",
      "[[0.68888058 0.31111942]\n",
      " [0.00150212 0.99849788]]\n"
     ]
    }
   ],
   "source": [
    "# Generate some new random values for two plants, one of each class\n",
    "new_a_height = numpy.random.normal(loc=PLANT_A_AVG_HEIGHT)\n",
    "new_a_width = numpy.random.normal(loc=PLANT_A_AVG_WIDTH)\n",
    "new_b_height = numpy.random.normal(loc=PLANT_B_AVG_HEIGHT)\n",
    "new_b_width = numpy.random.normal(loc=PLANT_B_AVG_WIDTH)\n",
    "\n",
    "# Pull the values into a matrix, because that is what the predict function wants\n",
    "inputs = [[new_a_height, new_a_width], [new_b_height, new_b_width]]\n",
    "\n",
    "# Print out the outputs for these new inputs\n",
    "print('Plant A: {0} {1}'.format(new_a_height, new_a_width))\n",
    "print('Plant B: {0} {1}'.format(new_b_height, new_b_width))\n",
    "print('Class predictions: {0}'.format(model.predict(inputs))) # guess which class\n",
    "print('Probabilities:\\n{0}'.format(model.predict_proba(inputs))) # give probability of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option #1 - Standard Difficulty\n",
    "\n",
    "Answer the following questions. You can also use the graph below, if seeing the data visually helps you understand the data.\n",
    "1. What should we be expecting as the output for class predictions in the above cell? If the model is not giving the expected output, what are some of the reasons it might not be?\n",
    "1. How do the probabilities output by the above cell relate to the class predictions? Why do you think the model might be more or less confident in its predictions?\n",
    "1. If you change the averages in the data generation code (like PLANT_A_AVG_HEIGHT) and re-run the code, how do the predictions change, and why?\n",
    "1. Looking at the intercept and coefficient output further above, if a coefficient is negative, what has the model learned about this feature? In other words, if you took a datapoint and you increased the value of a feature that has a negative coefficient, what would you expect to happen to the probabilities the model gives this datapoint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers for Option \\#1\n",
    "\n",
    "**1.**  \n",
    "The class predictions above are accurate; Plant A should be of Class 0 and Plant B should be of Class 1 because the test values (Plants A and B) were generated randomly using the same averages and Gaussian distribution that the data set was generated with. Because the test values are selected from a Gaussian distribution, it is unlikely that they will be far from the class averages and therefore should be easily identifiable as one class or the other. But if, for example, test case Plant A has height = 58.5 and width = 9 (unlikely), then the probabilites would show that Plant A would more likely be of Class 1 even though it was just a statistical fluke. Additionally (not in this case), it is possible that there could be more overlap between the two classes, which would make it hard for the model to give an accurate class prediction. However, in this case the probabilities would still be fairly accurate, but they would be much more ambigous (eg. ~40% vs ~60%).\n",
    "\n",
    "**2.**  \n",
    "I would assume that the class predictions would be chosen based on which probability is greater, so if it was 51% likely to be Class 0 and 49% likely to be Class 1, the class prediction would be 0. If the probabilities were tied by some miracle, I don't know what would happen (random choice, whichever class has a lower number, DNE?). There could also be a threshold value set up: for example, if we were trying to decide whether a tree was an oak or not, we could say that if the probability (that the tree is an oak) is greater than 75% than the tree is indeed an oak, but otherwise it's not. In this case, the model is quite confident of the class predictions becuase the probabilities are so strongly weighted (when I wrote this it was 0: 0.00302983, 1: 0.99697017). If the probabilities were more like (0: 0.4, 1: 0.6), the model would be less confident of the class predictions but I belive would still choose the same prediction.\n",
    "\n",
    "**3.**  \n",
    "Nothing fundamental will change, the numbers would shift a little but if the predictions/data are still based on the normal distribution curve and averages then the probabilites should still be heavily weighted and the predictions correct and confident. If you changned the averages of Class 0 and 1 to be closer together, then it would be harder to get a strong probability direction because there would be more overlap, and the confidence would decrease some.\n",
    "\n",
    "**4.**  \n",
    "Binomial logistic regression can be represented by the equation y = e^x / 1+e^x, where x = inputs\\*parameters = a\\*x1 + b\\*x2 + intercept. So as x goes to infinity y goes to 1, and as x goes to 0 so does the fraction. However, if we make a coefficient negative (a or b), it turns x negative too (or makes x smaller—which will have the same effect of decreasing the probability), which means that the equation becomes 1 / 1+e^x, which will get closer and closer to 0 as x goes to negative infinity. So, if you increase the magnitude of a negative coefficient, the probability is going to get smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option #3 - Advanced Difficulty\n",
    "\n",
    "If you have more than two classes, you can use multinomial logistic regression or the one vs. rest technique, where you use a binomial logistic regression for each class that you have and decide if it is or is not in that class. Try expanding the program with a third type and implementing your own one vs. rest models. To test if this is working, compare your output to running your expanded dataset through scikit-learn, which will automatically do one vs. rest if there are more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a third type\n",
    "\n",
    "\n",
    "PLANT_C_AVG_HEIGHT = 62.0\n",
    "PLANT_C_AVG_WIDTH = 12.0\n",
    "\n",
    "# Pick numbers randomly with a normal distribution centered around the averages\n",
    "\n",
    "plant_c_heights = numpy.random.normal(loc=PLANT_C_AVG_HEIGHT, size=NUM_INPUTS)\n",
    "plant_c_widths = numpy.random.normal(loc=PLANT_C_AVG_WIDTH, size=NUM_INPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vizualization\n",
    "\n",
    "\n",
    "# create a figure and label it\n",
    "fig = matplotlib.pyplot.figure()\n",
    "fig.suptitle('Three Classes Plant Data Set')\n",
    "matplotlib.pyplot.xlabel('Height')\n",
    "matplotlib.pyplot.ylabel('Width')\n",
    "\n",
    "# put the generated points on the graph\n",
    "a_scatter = matplotlib.pyplot.scatter(plant_a_heights, plant_a_widths, c=\"red\", marker=\"o\", label='plant a')\n",
    "b_scatter = matplotlib.pyplot.scatter(plant_b_heights, plant_b_widths, c=\"blue\", marker=\"^\", label='plant b')\n",
    "c_scatter = matplotlib.pyplot.scatter(plant_c_heights, plant_c_widths, c=\"green\", marker=\"*\", label='plant c')\n",
    "\n",
    "# add a legend to explain which points are which\n",
    "matplotlib.pyplot.legend(handles=[a_scatter, b_scatter, c_scatter])\n",
    "\n",
    "# show the graph\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create three different models\n",
    "\n",
    "To implement my own one vs. rest technique, I created three seperate binomial models. The first model would determine whether the test case fit into class a/0 by combining classes b/1 and c/2 into one class. The second model would use the same method (combining classes 0 and 2 into one class) to determine if the test case fit into class 1. The third would combine classes 1 and 2.\n",
    "\n",
    "To train each model, I created a unique classes list (solutions) for each. So for model a, my classes list comprised of 50 0s, then 100 1s. A 0 meant that plant was in class 0 and a 1 meant that plant was in class 1 or 2.\n",
    "\n",
    "**IMPORTANT:** In my code or documentation, \"class 0\" is the same as \"class a\", \"class 1\" is the same as \"class b\", and \"class 2\" is the same as \"class c\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create 3 models\n",
    "\n",
    "\n",
    "# this creates a 2-dimensional matrix, with heights in the first column and widths in the second\n",
    "# the first third of rows are all plants of type a, the second third are type b, and the last third are type c\n",
    "plant_inputs_3 = list(zip(numpy.append(plant_a_heights, numpy.append(plant_b_heights, plant_c_heights)),\n",
    "                        numpy.append(plant_a_widths, numpy.append(plant_b_widths, plant_c_widths))))\n",
    "\n",
    "classes_all = [0]*NUM_INPUTS + [1]*NUM_INPUTS + [2]*NUM_INPUTS # for using sklearn ovr later\n",
    "\n",
    "# class 0 is the class which is being tested to see if the test plant is part of that class, class 1 is everything else\n",
    "\n",
    "# for testing if the plant is part of class a/0\n",
    "classes_a = [0]*NUM_INPUTS + [1]*NUM_INPUTS + [1]*NUM_INPUTS\n",
    "model_a = linear_model.LogisticRegression()\n",
    "model_a.fit(plant_inputs_3, classes_a)\n",
    "\n",
    "# class b/1\n",
    "classes_b = [1]*NUM_INPUTS + [0]*NUM_INPUTS + [1]*NUM_INPUTS\n",
    "model_b = linear_model.LogisticRegression()\n",
    "model_b.fit(plant_inputs_3, classes_b)\n",
    "\n",
    "# class c/2\n",
    "classes_c = [1]*NUM_INPUTS + [1]*NUM_INPUTS + [0]*NUM_INPUTS\n",
    "model_c = linear_model.LogisticRegression()\n",
    "model_c.fit(plant_inputs_3, classes_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Plant A: 60.66758314739796 8.891691209171343\n",
      "Test Plant B: 58.939790859940594 11.522460314919316\n",
      "Test Plant C: 62.32184523936269 13.29152790357984\n"
     ]
    }
   ],
   "source": [
    "# create test values\n",
    "\n",
    "\n",
    "# Generate a single new plant for each class\n",
    "test_a_height = numpy.random.normal(loc=PLANT_A_AVG_HEIGHT)\n",
    "test_a_width = numpy.random.normal(loc=PLANT_A_AVG_WIDTH)\n",
    "test_b_height = numpy.random.normal(loc=PLANT_B_AVG_HEIGHT)\n",
    "test_b_width = numpy.random.normal(loc=PLANT_B_AVG_WIDTH)\n",
    "test_c_height = numpy.random.normal(loc=PLANT_C_AVG_HEIGHT)\n",
    "test_c_width = numpy.random.normal(loc=PLANT_C_AVG_WIDTH)\n",
    "\n",
    "# Print out the test values\n",
    "print('Test Plant A: {0} {1}'.format(test_a_height, test_a_width))\n",
    "print('Test Plant B: {0} {1}'.format(test_b_height, test_b_width))\n",
    "print('Test Plant C: {0} {1}'.format(test_c_height, test_c_width))\n",
    "\n",
    "# Pull the values into a matrix, because that is what the predict function wants\n",
    "inputs = [[[test_a_height, test_a_width]], [[test_b_height, test_b_width]], [[test_c_height, test_c_width]]]\n",
    "print_statements = [\"Test Plant A:\", \"\\nTest Plant B:\", \"\\nTest Plant C:\"] # this is for use in the for loop (next code cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Here I go through each test case (a, b, and c) using a for loop. Inside that loop I run each model over the current test case to get predictions and probabilities. If a prediction is 0, it means that the model believes that the test case is of the class linked to the model. This is because when I trained the models, I used 0 to represent one of the three classes and 1 to represent the other two. Therefore, the prediction will be 0 if the test case is of that model's class. ```model_a``` is linked to class a (or 0), ```model_b``` is linked to class b (or 1), and ```model_c``` is linked to class c (or 2).\n",
    "\n",
    "Sometimes the models will output two classes as correct. This is because I'm testing each case individually whether it fits into one class of not, so theoretically the test case could fit into overlap between two classes and be predicted as both seperately. To fix this, I additionally compare the percentages that the test case is part of class 0 (the class linked to the model) and output the class which has the largest percentage.\n",
    "\n",
    "At the end, I also use a built-in sklearn ovr model to double check my results (even though I can already figure out if they're correct or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Plant A:\n",
      "Class predictions (0 is true and 1 is false): a:[0] b:[1] c:[1]\n",
      "Probabilities: a:[[0.86739218 0.13260782]] b:[[0.00831635 0.99168365]] c:[[0.02574201 0.97425799]]\n",
      "Based on all three probabilities this test is of class a\n",
      "\n",
      "Test Plant B:\n",
      "Class predictions (0 is true and 1 is false): a:[1] b:[0] c:[1]\n",
      "Probabilities: a:[[0.00342111 0.99657889]] b:[[0.7425365 0.2574635]] c:[[0.22585417 0.77414583]]\n",
      "Based on all three probabilities this test is of class b\n",
      "\n",
      "Test Plant C:\n",
      "Class predictions (0 is true and 1 is false): a:[1] b:[1] c:[0]\n",
      "Probabilities: a:[[0.0021581 0.9978419]] b:[[0.00160207 0.99839793]] c:[[0.9995498 0.0004502]]\n",
      "Based on all three probabilities this test is of class c\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "\n",
    "\n",
    "for j in range(len(inputs)): # go through each test case\n",
    "    \n",
    "    i = inputs[j]\n",
    "    p = print_statements[j]\n",
    "    \n",
    "    # get predictions from each model\n",
    "    pred_a = model_a.predict(i)\n",
    "    pred_b = model_b.predict(i)\n",
    "    pred_c = model_c.predict(i)\n",
    "    \n",
    "    # get probabilities from each model\n",
    "    prob_a = model_a.predict_proba(i)\n",
    "    prob_b = model_b.predict_proba(i)\n",
    "    prob_c = model_c.predict_proba(i)\n",
    "    \n",
    "    print(p)\n",
    "    print(\"Class predictions (0 is true and 1 is false): a:{0} b:{1} c:{2}\".format(pred_a, pred_b, pred_c))\n",
    "    print(\"Probabilities: a:{0} b:{1} c:{2}\".format(prob_a, prob_b, prob_c))\n",
    "    \n",
    "    # to account if two models think the test case is part of two seperate classes\n",
    "    \n",
    "    prob_a_true = float(str(prob_a[0][0]).split()[0]) # get the probability that the test case is of class a\n",
    "    prob_b_true = float(str(prob_b[0][0]).split()[0]) # class b\n",
    "    prob_c_true = float(str(prob_c[0][0]).split()[0]) # class c\n",
    "    \n",
    "    if prob_a_true > prob_b_true and prob_a_true > prob_c_true: # if prob of a is biggest\n",
    "        print(\"Based on all three probabilities this test is of class a\")\n",
    "        \n",
    "    elif prob_b_true > prob_c_true and prob_b_true > prob_a_true: # if prob of b is biggest\n",
    "        print(\"Based on all three probabilities this test is of class b\")\n",
    "        \n",
    "    elif prob_c_true > prob_a_true and prob_c_true > prob_b_true: # if prob of c is biggest\n",
    "        print(\"Based on all three probabilities this test is of class c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Plant A:\n",
      "Class prediction: [0]\n",
      "Probabilities for class [a b c]: [[0.96221827 0.00922552 0.02855621]]\n",
      "\n",
      "Test Plant B:\n",
      "Class prediction: [1]\n",
      "Probabilities for class [a b c]: [[0.00352035 0.7640744  0.23240526]]\n",
      "\n",
      "Test Plant C:\n",
      "Class prediction: [2]\n",
      "Probabilities for class [a b c]: [[0.00215098 0.00159679 0.99625223]]\n"
     ]
    }
   ],
   "source": [
    "# double check using sklearn ovr (even though my code works correctly)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = LogisticRegression(random_state=0, multi_class='ovr')\n",
    "model_ovr = clf.fit(plant_inputs_3, classes_all)\n",
    "\n",
    "for j in range(len(inputs)): # go through each test case\n",
    "    \n",
    "    i = inputs[j]\n",
    "    p = print_statements[j]\n",
    "    \n",
    "    # get prediction\n",
    "    pred = model_ovr.predict(i)\n",
    "    \n",
    "    # get probability\n",
    "    prob = model_ovr.predict_proba(i)\n",
    "    \n",
    "    print(p)\n",
    "    print(\"Class prediction: {0}\".format(pred))\n",
    "    print(\"Probabilities for class [a b c]: {0}\".format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test values\n",
    "# I create 1000 test values for each class to ensure robust metrics\n",
    "\n",
    "\n",
    "values = []\n",
    "classes_true = []\n",
    "\n",
    "for i in range(1000): # test values of class a\n",
    "    h = numpy.random.normal(loc=PLANT_A_AVG_HEIGHT)\n",
    "    w = numpy.random.normal(loc=PLANT_A_AVG_WIDTH)\n",
    "    values.append([[h, w]])\n",
    "    classes_true.append(0)\n",
    "    \n",
    "for i in range(1000): # class b\n",
    "    h = numpy.random.normal(loc=PLANT_B_AVG_HEIGHT)\n",
    "    w = numpy.random.normal(loc=PLANT_B_AVG_WIDTH)\n",
    "    values.append([[h, w]])\n",
    "    classes_true.append(1)\n",
    "    \n",
    "for i in range(1000): # class c\n",
    "    h = numpy.random.normal(loc=PLANT_C_AVG_HEIGHT)\n",
    "    w = numpy.random.normal(loc=PLANT_C_AVG_WIDTH)\n",
    "    values.append([[h, w]])\n",
    "    classes_true.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict each test value and create a list of class predictions\n",
    "\n",
    "\n",
    "classes_pred = []\n",
    "classes_prob = []\n",
    "    \n",
    "for t in values:\n",
    "\n",
    "    # get predictions from each model\n",
    "    pred_a = model_a.predict(t)\n",
    "    pred_b = model_b.predict(t)\n",
    "    pred_c = model_c.predict(t)\n",
    "\n",
    "    # get probabilities from each model\n",
    "    prob_a = model_a.predict_proba(t)\n",
    "    prob_b = model_b.predict_proba(t)\n",
    "    prob_c = model_c.predict_proba(t)\n",
    "\n",
    "    # to account if two models think the test case is part of two seperate classes\n",
    "    \n",
    "    prob_a_true = float(str(prob_a[0][0]).split()[0]) # get the probability that the test case is of class a\n",
    "    prob_b_true = float(str(prob_b[0][0]).split()[0]) # class b\n",
    "    prob_c_true = float(str(prob_c[0][0]).split()[0]) # class c\n",
    "\n",
    "    if prob_a_true > prob_b_true and prob_a_true > prob_c_true: # if prob of a is biggest\n",
    "        classes_pred.append(0)\n",
    "        \n",
    "    elif prob_b_true > prob_c_true and prob_b_true > prob_a_true: # if prob of b is biggest\n",
    "        classes_pred.append(1)\n",
    "        \n",
    "    elif prob_c_true > prob_a_true and prob_c_true > prob_b_true: # if prob of c is biggest\n",
    "        classes_pred.append(2)\n",
    "        \n",
    "    dif = 1 - prob_a_true - prob_b_true - prob_c_true\n",
    "    \n",
    "    dif_split = dif/3\n",
    "    \n",
    "    prob_a_true += dif_split\n",
    "    prob_b_true += dif_split\n",
    "    prob_b_true += dif_split\n",
    "    \n",
    "    classes_prob.append([prob_a_true, prob_b_true, prob_c_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for class [a, b, c]: [0.88224122 0.92315789 0.97291876]\n",
      "avg precision (calculated using average='micro'): 0.9253333333333333\n",
      "avg precision (calculated using (a+b+c)/3): 0.9261059566666668\n",
      "\n",
      "recall for class [a, b, c]: [0.929 0.877 0.97 ]\n",
      "avg recall (calculated using average='micro'): 0.9253333333333333\n",
      "avg recall (calculated using (a+b+c)/3): 0.9253333333333332\n",
      "\n",
      "f1 score for class [a, b, c]: [0.90501705 0.89948718 0.97145719]\n",
      "avg f1 score (calculated using average='micro'): 0.9253333333333333\n",
      "avg f1 score (calculated using (a+b+c)/3): 0.9253204733333332\n",
      "\n",
      "accuracy score: 0.9253333333333333\n",
      "\n",
      "AUC score (multiclass='ovr'): 0.9886303333333334\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall, f1 score, and accuracy using the built in sklearn functions\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "\n",
    "# precision\n",
    "\n",
    "p = precision_score(classes_true, classes_pred, average=None)\n",
    "print(\"precision for class [a, b, c]:\", p)\n",
    "print(\"avg precision (calculated using average=\\'micro\\'):\", precision_score(classes_true, classes_pred, average=\"micro\"))\n",
    "\n",
    "# compute average precision using the precision from each class (average=None)\n",
    "total = 0\n",
    "for n in re.findall(\"[\\d\\.]+\", str(p)):\n",
    "    total += float(n)\n",
    "print(\"avg precision (calculated using (a+b+c)/3):\", total/3)\n",
    "\n",
    "# recall\n",
    "\n",
    "r = recall_score(classes_true, classes_pred, average=None)\n",
    "print(\"\\nrecall for class [a, b, c]:\", r)\n",
    "print(\"avg recall (calculated using average=\\'micro\\'):\", recall_score(classes_true, classes_pred, average=\"micro\"))\n",
    "\n",
    "# compute average recall using the recall from each class (average=None)\n",
    "total = 0\n",
    "for n in re.findall(\"[\\d\\.]+\", str(r)):\n",
    "    total += float(n)\n",
    "print(\"avg recall (calculated using (a+b+c)/3):\", total/3)\n",
    "\n",
    "# f1 score\n",
    "\n",
    "f1 = f1_score(classes_true, classes_pred, average=None)\n",
    "print(\"\\nf1 score for class [a, b, c]:\", f1)\n",
    "print(\"avg f1 score (calculated using average=\\'micro\\'):\", f1_score(classes_true, classes_pred, average=\"micro\"))\n",
    "\n",
    "# compute average f1 score using the f1 score from each class (average=None)\n",
    "total = 0\n",
    "for n in re.findall(\"[\\d\\.]+\", str(f1)):\n",
    "    total += float(n)\n",
    "print(\"avg f1 score (calculated using (a+b+c)/3):\", total/3)\n",
    "\n",
    "# accuracy\n",
    "\n",
    "print(\"\\naccuracy score:\", accuracy_score(classes_true, classes_pred))\n",
    "\n",
    "# AUC score\n",
    "# https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "\n",
    "print(\"\\nAUC score (multiclass=\\'ovr\\'):\", roc_auc_score(classes_true, classes_prob, multi_class=\"ovr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How I made the ovr model for AUC:\n",
    "\n",
    "First, based on the true class, I appended to the lists `a_true`, `b_true`, or `c_true` zeros or ones. I appended a 0 to the list which corresponded to the true class (a=0, b=1, c=2) and 1s to the other two lists, recording that the test case was not the class of that list.\n",
    "\n",
    "If the prediction was true, I appended a 0 to the prediction list (`a_pred`, `b_pred`, `c_pred`) which corresponded with that class—matching the 0 in the true list. I appended 1s to the other two lists to similarly match the true lists (another way to say that is the model correctly predicted that the class was not the one corresponding to that list).\n",
    "\n",
    "If the prediction was false I just did the opposite to the true prediction.\n",
    "\n",
    "### What I think of my results:\n",
    "\n",
    "I think that these results are actually pretty darn good, it is hard to get both a high recall and precision and I think in this case it is best to have both high recall and precision rather than optimizing for one, as there is no need to catch every single plant of class a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was my first iteration (I tried to compute tp, fp, tn, and fn by hand)\n",
    "# I spent long enough on this I deemed it worthy of saving just in case I ever wanted it again\n",
    "\n",
    "# tp = {\"a\": 0, \"b\": 0, \"c\": 0}\n",
    "# fp = {\"a\": 0, \"b\": 0, \"c\": 0}\n",
    "# tn = {\"a\": 0, \"b\": 0, \"c\": 0}\n",
    "# fn = {\"a\": 0, \"b\": 0, \"c\": 0}\n",
    "\n",
    "# if prob_a_true > prob_b_true and prob_a_true > prob_c_true: # of class a/0\n",
    "#         if c == 0:\n",
    "#             tp[\"a\"] += 1\n",
    "#             tn[\"b\"] += 1\n",
    "#             tn[\"c\"] += 1\n",
    "#         elif c == 1:\n",
    "#             fp[\"a\"] += 1\n",
    "#             fn[\"b\"] += 1\n",
    "#             tn[\"c\"] += 1\n",
    "#         elif c == 2:\n",
    "#             fp[\"a\"] += 1\n",
    "#             tn[\"b\"] += 1\n",
    "#             fn[\"c\"] += 1\n",
    "#         else:\n",
    "#             print(\"error!\")\n",
    "# elif prob_b_true > prob_c_true and prob_b_true > prob_a_true: # of class b/1\n",
    "#     if c == 0:\n",
    "#         fp[\"b\"] += 1\n",
    "#         fn[\"a\"] += 1\n",
    "#         tn[\"c\"] += 1\n",
    "#     elif c == 1:\n",
    "#         tp[\"b\"] += 1\n",
    "#         tn[\"c\"] += 1\n",
    "#         tn[\"a\"] += 1\n",
    "#     elif c == 2:\n",
    "#         fp[\"b\"] += 1\n",
    "#         fn[\"c\"] += 1\n",
    "#         tn[\"a\"] += 1\n",
    "#     else:\n",
    "#         print(\"error!\")\n",
    "# elif prob_c_true > prob_a_true and prob_c_true > prob_b_true: # of class c/2\n",
    "#     if c == 0:\n",
    "#         fp[\"c\"] += 1\n",
    "#         fn[\"a\"] += 1\n",
    "#         tn[\"b\"] += 1\n",
    "#     elif c == 1:\n",
    "#         fp[\"c\"] += 1\n",
    "#         fn[\"b\"] += 1\n",
    "#         tn[\"a\"] += 1\n",
    "#     elif c == 2:\n",
    "#         tp[\"c\"] += 1\n",
    "#         tn[\"a\"] += 1\n",
    "#         tn[\"b\"] += 1\n",
    "#     else:\n",
    "#         print(\"error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old failed AUC score using hand-made ovr\n",
    "# I used the predictions instead of the probabilities\n",
    "\n",
    "# AUC = {\"a_true\" : [], \"a_pred\" : [], \"b_true\" : [], \"b_pred\" : [], \"c_true\" : [], \"c_pred\" : []}\n",
    "\n",
    "# for i in range(len(classes_true)):\n",
    "    \n",
    "#     t = classes_true[i]\n",
    "#     p = classes_pred[i]\n",
    "    \n",
    "#     if t == 0: # if true class is 0\n",
    "        \n",
    "#         AUC[\"a_true\"].append(0)\n",
    "#         AUC[\"b_true\"].append(1)\n",
    "#         AUC[\"c_true\"].append(1)\n",
    "        \n",
    "#         if p == 0:\n",
    "            \n",
    "#             # true predictions for everything\n",
    "#             AUC[\"a_pred\"].append(0)\n",
    "#             AUC[\"b_pred\"].append(1)\n",
    "#             AUC[\"c_pred\"].append(1)\n",
    "        \n",
    "#         else:\n",
    "            \n",
    "#             # false predictions for everything\n",
    "#             AUC[\"a_pred\"].append(1)\n",
    "#             AUC[\"b_pred\"].append(0)\n",
    "#             AUC[\"c_pred\"].append(0)\n",
    "            \n",
    "#     elif t == 1: # if true class is 1\n",
    "        \n",
    "#         AUC[\"a_true\"].append(1)\n",
    "#         AUC[\"b_true\"].append(0)\n",
    "#         AUC[\"c_true\"].append(1)\n",
    "        \n",
    "#         if p == 1:\n",
    "            \n",
    "#             # true predictions for everything\n",
    "#             AUC[\"a_pred\"].append(1)\n",
    "#             AUC[\"b_pred\"].append(0)\n",
    "#             AUC[\"c_pred\"].append(1)\n",
    "            \n",
    "#         else:\n",
    "            \n",
    "#             # false predictions for everything\n",
    "#             AUC[\"a_pred\"].append(0)\n",
    "#             AUC[\"b_pred\"].append(1)\n",
    "#             AUC[\"c_pred\"].append(0)\n",
    "        \n",
    "#     elif t == 2: # if true class is 2\n",
    "        \n",
    "#         AUC[\"a_true\"].append(1)\n",
    "#         AUC[\"b_true\"].append(1)\n",
    "#         AUC[\"c_true\"].append(0)\n",
    "        \n",
    "#         if p == 2:\n",
    "            \n",
    "#             # true predictions for everything\n",
    "#             AUC[\"a_pred\"].append(1)\n",
    "#             AUC[\"b_pred\"].append(1)\n",
    "#             AUC[\"c_pred\"].append(0)\n",
    "            \n",
    "#         else:\n",
    "            \n",
    "#             # false predictions for everything\n",
    "#             AUC[\"a_pred\"].append(0)\n",
    "#             AUC[\"b_pred\"].append(0)\n",
    "#             AUC[\"c_pred\"].append(1)\n",
    "            \n",
    "# AUC_ab = roc_auc_score(AUC[\"a_true\"], AUC[\"a_pred\"])\n",
    "# AUC_bc = roc_auc_score(AUC[\"b_true\"], AUC[\"b_pred\"])\n",
    "# AUC_ca = roc_auc_score(AUC[\"c_true\"], AUC[\"c_pred\"])\n",
    "        \n",
    "# print(\"\\nAUC score for classes a, b:\", AUC_ab)\n",
    "# print(\"AUC score for classes b, c:\", AUC_bc)\n",
    "# print(\"AUC score for classes c, a:\", AUC_ca)\n",
    "\n",
    "# print(\"avg AUC score:\", (AUC_ab + AUC_bc + AUC_ca)/3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
